📺 The best Stanford, CMU, and MIT courses for AI (with YouTube playlists)

- With a multitude of AI courses available online, coming up with an study plan for AI can easily lead to decision fatigue.
- I often get asked about which courses have been useful to me to build a foundation in AI. Here’s my list of courses along with their respective YouTube playlists.
- Check out my watch list with all of the below pointers (and a much larger list of such resources and more): https://aman.ai/watch

📚 Stanford University
🔹 CS221 - Artificial Intelligence: Principles and Techniques by Percy Liang and Dorsa Sadigh: https://lnkd.in/grECwbD4
🔹 CS229 - Machine Learning by Andrew Ng: https://lnkd.in/gY8a2yZN
🔹 CS230 - Deep Learning by Andrew Ng: https://lnkd.in/gTk-gKPm
🔹 CS231n - Convolutional Neural Networks for Visual Recognition by Fei-Fei Li and Andrej Karpathy: https://lnkd.in/gGUMZH_G
🔹 CS224n - Natural Language Processing with Deep Learning by Christopher Manning: https://lnkd.in/giWDZGVX
🔹 CS224w - Machine Learning with Graphs by Jure Leskovec: https://lnkd.in/gQran26Z
🔹 CS234 - Reinforcement Learning by Emma Brunskill: https://lnkd.in/gwZKQ-28
🔹 CS330 - Deep Multi-task and Meta Learning by Chelsea Finn: https://lnkd.in/gvVr_Y4M
🔹 CS25 - Transformers United by Divyansh Garg, Steven Feng, and Rylan Schaeffer: https://lnkd.in/gEtKgHGC

📚 Carnegie Mellon University
🔹 CS/LTI 11-711: Advanced NLP by Graham Neubig: https://lnkd.in/gSt29ZVt
🔹 CS/LTI 11-747: Neural Networks for NLP by Graham Neubig: https://lnkd.in/gRRrY8uq
🔹 CS/LTI 11-737: Multilingual NLP by Graham Neubig: https://lnkd.in/g8QkaTfy
🔹 CS/LTI 11-777: Multimodal Machine Learning by Louis-Philippe Morency: https://lnkd.in/gKFJDbU4
🔹 CS/LTI 11-785: Introduction to Deep Learning by Bhiksha Raj and Rita Singh:
https://lnkd.in/gVp96GdB
🔹 CS/LTI Low Resource NLP Bootcamp 2020 by Graham Neubig: https://lnkd.in/grYqa3YZ

📚 Massachusetts Institute of Technology
🔹 6.S191 - Introduction to Deep Learning by Alexander Amini and Ava Amini: https://lnkd.in/gWMUpMQg
🔹 6.S094 - Deep Learning by Lex Fridman: https://lnkd.in/gcDgqbH6
🔹 6.S192 - Deep Learning for Art, Aesthetics, and Creativity by Ali Jahanian: https://lnkd.in/gEyRbEZx

📚 University College London (UCL)
🔹 COMP M050 Reinforcement Learning by David Silver: https://lnkd.in/gEpkWmqh



MLOps is the DevOps for machine learning.

It streamlines the end-to-end machine learning lifecycle by integrating best practices from the software development world.

This ensures that machine learning models are not just built, but are also effectively deployed, monitored, and managed in production.

It includes:
1. Continuous Integration (CI): Automated testing of ML code and pipelines.
2. Continuous Delivery (CD): Automate model deployment processes.
3. Model Versioning: Track and manage different versions of models and data.
4. Model Monitoring: Keep an eye on model performance and health in real-time.
5. Model Retraining: Ensure models stay relevant by retraining them with new data.
6. Scalability & Serving: Efficiently serve models to handle real-world traffic.
7. Collaboration: Tools for team collaboration and reproducibility across ML workflows.

MLOps ensures reliable and faster delivery of high-quality ML models to production.
↓
https://media.licdn.com/dms/image/D5622AQHv1d7ajrjPPg/feedshare-shrink_2048_1536/0/1691516134239?e=1694649600&v=beta&t=Gm7lgf7vQGff910pxXw75oYz84iTUzE4NlYZGWiqK0o 

Notebooks for finetuning and inference of Llama-2 and LLMs

- bnb-4bit-training-with-inference
https://lnkd.in/dtiky_k8

- Llama 2 Fine-Tuning using QLora
https://lnkd.in/dEd4xTAR

- Fine-tune Llama 2 in Google Colab
https://lnkd.in/dHi2JHcf

- llama-2-70b-chat-agent
https://lnkd.in/dWjSdqRB

- text-generation-inference
https://lnkd.in/dvmMGhD6

- text-generation-webui
https://lnkd.in/dQ_NVNkP

- llama2-webui
https://lnkd.in/dkY3c_Pc

- llama_cpu_interface
https://lnkd.in/dyq2ft-q


########
AI is taking over the world. And, you can either take a backseat. Or, invest your time in learning it. Here's my 𝟱-𝘀𝘁𝗲𝗽 𝗳𝗼𝗿𝗺𝘂𝗹𝗮 📚 on how I gain deep intuition of 𝗦𝗢𝗧𝗔𝘀 𝗹𝗶𝗸𝗲 𝗦𝘁𝗮𝗯𝗹𝗲 𝗗𝗶𝗳𝗳𝘂𝘀𝗶𝗼𝗻 and 𝗟𝗮𝗿𝗴𝗲 𝗟𝗮𝗻𝗴𝘂𝗮𝗴𝗲 𝗠𝗼𝗱𝗲𝗹𝘀 👇

𝗦𝘁𝗲𝗽 𝟭 - 💡 𝗚𝗮𝘁𝗵𝗲𝗿 𝗜𝗻𝗴𝗿𝗲𝗱𝗶𝗲𝗻𝘁𝘀 𝗤𝘂𝗶𝗰𝗸𝗹𝘆

When you want to cook a dish, what's the first step? You have to know what ingredients to pick up.

To understand a SOTA model like Stable Diffusion, start by gathering the essential components that make up the algo.

- Forward Diffusion Process
- Variance Scheduling
- Markov Chain
- Reverse Diffusion Process
- UNet
- Parameter Sampling
- Attention Mechanism

Once you have your "ingredient", you now have a map of the building blocks you need to understand how the pieces fit together for the overall model process.

𝗦𝘁𝗲𝗽 𝟮 - ✍️ 𝗗𝘄𝗲𝗹𝗹 𝗼𝗻 𝘁𝗵𝗲 𝗺𝗮𝘁𝗵

I can't emphasize this enough. Knowing how to apply the algorithm with a Tensorflow library is just the tip of the iceberg.

If you want a deep intuition of SOTA model, find a coffee shop and work out the math in matrix form step-by-step.

𝗦𝘁𝗲𝗽 𝟯 - ⌨️ 𝗖𝗼𝗱𝗲 𝗶𝘁

Import tensorflow or import torch, and play around with the algorithm on a toy problem.

Browse Githubs, Medium, Kaggle, or Hugging Face for implementations.

Copy the code, play around with the parameter values, and input a new dataset and observe the output.

𝗦𝘁𝗲𝗽 𝟰 - ✅ 𝗖𝗼𝗱𝗲 𝗶𝘁 𝗳𝗿𝗼𝗺 𝘀𝗰𝗿𝗮𝘁𝗰𝗵

Andrej Karpathy shares this a lot, and it's also something I've been doing since I became a practitioner in ML 7 years ago. And, it's what I've done for all the popular types of ML algos (e.g. XGBoost, RNN, Diffusion Models)

Code it from scratch - see how far you could go in each level.

Level 1 - Use tensorflow/torch
Level 2 - Only use numpy
Level 3 - Only use vanilla python

𝗦𝘁𝗲𝗽 𝟱 - 📝 𝗧𝗲𝗮𝗰𝗵 𝗶𝘁

The best way to learn is to teach. Write about it. Talk about it. Share your learning with the world so others could learn.

(𝗦𝘁𝗲𝗽 𝟲) - 🎯 𝗥𝗲𝗽𝗲𝗮𝘁 𝗶𝘁 𝗳𝗼𝗿 𝘁𝗵𝗲 𝗻𝗲𝘅𝘁 𝗦𝗢𝗧𝗔

Learning never stops. Continue to make progress in growing your knowledge in ML/AI.

👉 What's the algo you are learning right now? Drop one below 👇
👉 Land dream data job 🌈 on 𝗗𝗮𝘁𝗮𝗜𝗻𝘁𝗲𝗿𝘃𝗶𝗲𝘄[.]𝗰𝗼𝗺 🚀
👉 Found this post helpful? Smash 👍 and follow Daniel Lee 📚